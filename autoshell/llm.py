import json
import re
import time
from openai import OpenAI
from rich.console import Console
from .config import Config

console = Console()

class LLMClient:
    def __init__(self):
        Config.validate()
        
        # æ£€æµ‹æä¾›å•†ç±»å‹
        self.is_ollama = Config.is_ollama()
        provider_name = "Ollama (Local)" if self.is_ollama else "OpenAI Compatible"
        
        console.print(f"[dim][DEBUG] Initializing LLM Client...[/dim]")
        console.print(f"[dim][DEBUG] Provider: {provider_name}[/dim]")
        console.print(f"[dim][DEBUG] API Base URL: {Config.OPENAI_BASE_URL}[/dim]")
        console.print(f"[dim][DEBUG] Model: {Config.LLM_MODEL}[/dim]")
        
        # å®‰å…¨æ˜¾ç¤ºAPI Keyï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸æ˜¯ Ollamaï¼‰
        if not self.is_ollama and Config.OPENAI_API_KEY and Config.OPENAI_API_KEY != "not-needed":
            masked_key = f"{Config.OPENAI_API_KEY[:10]}...{Config.OPENAI_API_KEY[-4:]}"
            console.print(f"[dim][DEBUG] API Key: {masked_key}[/dim]")
        
        try:
            self.client = OpenAI(
                api_key=Config.OPENAI_API_KEY,
                base_url=Config.OPENAI_BASE_URL,
                timeout=30.0  # æ·»åŠ 30ç§’è¶…æ—¶
            )
            console.print(f"[dim][DEBUG] Client initialized successfully[/dim]")
        except Exception as e:
            console.print(f"[bold red][DEBUG] Failed to initialize client: {str(e)}[/bold red]")
            raise
        
        self.model = Config.LLM_MODEL

    def _clean_json_response(self, content: str) -> str:
        """
        æ¸…ç† LLM å¯èƒ½è¿”å›çš„ Markdown ä»£ç å—æ ‡è®°ï¼Œæå–çº¯ JSON å­—ç¬¦ä¸²ã€‚
        æ”¯æŒå¤šç§æ ¼å¼çš„å“åº”ã€‚
        """
        content = content.strip()
        
        # 1. ç§»é™¤ ```json ... ``` æˆ– ``` ... ``` åŒ…è£¹
        match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", content)
        if match:
            content = match.group(1).strip()
        
        # 2. æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡ {...}
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ª{ï¼Œç„¶ååŒ¹é…å¯¹åº”çš„}
        first_brace = content.find('{')
        if first_brace == -1:
            return content
        
        # ä»ç¬¬ä¸€ä¸ª{å¼€å§‹ï¼Œè®¡æ•°æ‹¬å·æ¥æ‰¾åˆ°åŒ¹é…çš„}
        brace_count = 0
        in_string = False
        escape_next = False
        
        for i in range(first_brace, len(content)):
            char = content[i]
            
            # å¤„ç†å­—ç¬¦ä¸²ä¸­çš„å¼•å·
            if escape_next:
                escape_next = False
                continue
            
            if char == '\\':
                escape_next = True
                continue
            
            if char == '"':
                in_string = not in_string
                continue
            
            # åªåœ¨éå­—ç¬¦ä¸²ä¸­è®¡æ•°æ‹¬å·
            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        # æ‰¾åˆ°åŒ¹é…çš„}ï¼Œæå–å®Œæ•´çš„JSONå¯¹è±¡
                        return content[first_brace:i+1]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„}ï¼Œè¿”å›ä»ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}
        last_brace = content.rfind('}')
        if last_brace > first_brace:
            return content[first_brace:last_brace+1]
        
        return content.strip()

    def generate_plan(self, user_query: str, context_str: str, error_history: list | None = None) -> dict:
        """
        æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å’Œç¯å¢ƒä¸Šä¸‹æ–‡ç”Ÿæˆ Shell å‘½ä»¤è®¡åˆ’ã€‚
        
        :param user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤
        :param context_str: æ ¼å¼åŒ–åçš„ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
        :param error_history: ä¹‹å‰çš„é”™è¯¯å†å²ï¼Œç”¨äºé‡è¯•/è‡ªæ„ˆé€»è¾‘
        :return: è§£æåçš„ JSON å­—å…¸ {"thought": ..., "steps": [{"description":..., "command":...}, ...]}
        """
        
        console.print(f"[dim][DEBUG] Starting plan generation for query: {user_query[:50]}...[/dim]")
        start_time = time.time()
        
        system_prompt = f"""
You are an expert system engineer and command-line wizard.
Your goal is to translate natural language instructions into a SERIES of precise, efficient, and safe Shell commands.

Current Execution Environment:
{context_str}

âš ï¸ CRITICAL JSON FORMAT REQUIREMENTS âš ï¸

YOU MUST RESPOND WITH **ONLY** A VALID JSON OBJECT IN THIS **EXACT** FORMAT:

{{
   "thought": "Brief explanation of the plan",
   "steps": [
      {{
         "description": "Step description",
         "command": "shell command"
      }}
   ]
}}

ğŸš« FORBIDDEN:
- NO text before or after the JSON
- NO markdown code blocks (no ```)
- NO explanations outside the JSON
- NO conversational text
- NO other JSON structures (like {{"type":"shell"}} or {{"args":[]}})

âœ… REQUIRED FIELDS:
- "thought": string - Your reasoning (required)
- "steps": array - List of command steps (required, must have at least 1 step)
  - Each step MUST have:
    - "description": string - What this step does
    - "command": string - The shell command to execute

ğŸ“‹ EXAMPLES:

Example 1 - Simple command "show current directory":
{{
   "thought": "Execute pwd command to show current working directory",
   "steps": [
      {{
         "description": "Display current directory",
         "command": "pwd"
      }}
   ]
}}

Example 2 - Multiple steps "list files and count them":
{{
   "thought": "First list all files, then count the number of files",
   "steps": [
      {{
         "description": "List all files in current directory",
         "command": "ls -la"
      }},
      {{
         "description": "Count number of files",
         "command": "ls -1 | wc -l"
      }}
   ]
}}

ğŸ”§ EXECUTION RULES:
1. Analyze the user's request based on the current OS and Shell
2. Break down the task into sequential logical steps
3. For each step, formulate a valid shell command for the detected Shell type
4. Use Windows commands (like 'dir', 'cd') for Windows/PowerShell
5. Use Unix commands (like 'ls', 'pwd') for Unix/Linux/Mac
6. 'cd' commands will be handled specially by the execution engine

âš ï¸ REMEMBER: Output ONLY the JSON object - absolutely nothing else!
"""

        user_message = f"""User Request: {user_query}

IMPORTANT: You MUST respond with ONLY a JSON object in this exact format:
{{
   "thought": "your reasoning here",
   "steps": [
      {{"description": "step description", "command": "shell command"}}
   ]
}}

Do NOT include any other text, explanations, or markdown. ONLY the JSON object."""

        if error_history:
            # error_history ç»“æ„: [{"step_index": int, "command": str, "error": str}, ...]
            error_context = "\n".join([f"Previous failure at step {e.get('step_index', '?')}:\nCommand: {e['command']}\nError: {e['error']}" for e in error_history])
            user_message += f"\n\nPREVIOUS EXECUTION FAILED. Please analyze the errors and provide a FIXED plan (you can adjust the remaining steps):\n{error_context}"

        raw_content = None  # åˆå§‹åŒ–å˜é‡ä»¥é¿å…æœªç»‘å®šè­¦å‘Š
        
        try:
            console.print(f"[dim][DEBUG] Calling LLM API with model: {self.model}[/dim]")
            
            # æ„å»ºAPIè°ƒç”¨å‚æ•°
            api_params = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
            }
            
            # å¤„ç† JSON æ¨¡å¼
            response = None
            
            if self.is_ollama:
                # Ollama: ä¸ä½¿ç”¨ JSON æ¨¡å¼ï¼Œä¾èµ– prompt engineering
                console.print(f"[dim][DEBUG] Using Ollama, relying on prompt for JSON output[/dim]")
                response = self.client.chat.completions.create(**api_params)
            else:
                # é Ollama: å°è¯•ä½¿ç”¨ JSON æ¨¡å¼
                json_mode_failed = False
                
                try:
                    api_params["response_format"] = {"type": "json_object"}
                    console.print(f"[dim][DEBUG] Attempting to enable JSON mode for model: {self.model}[/dim]")
                    response = self.client.chat.completions.create(**api_params)
                except Exception as e:
                    error_msg = str(e)
                    if "response_format" in error_msg or "400" in error_msg:
                        console.print(f"[dim][DEBUG] JSON mode not supported by this API, retrying without it...[/dim]")
                        json_mode_failed = True
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                        raise
                
                # å¦‚æœJSONæ¨¡å¼å¤±è´¥ï¼Œé‡è¯•ä¸å¸¦JSONæ¨¡å¼
                if json_mode_failed:
                    api_params.pop("response_format", None)
                    console.print(f"[dim][DEBUG] Calling API without JSON mode...[/dim]")
                    response = self.client.chat.completions.create(**api_params)
            
            elapsed = time.time() - start_time
            console.print(f"[dim][DEBUG] LLM API responded in {elapsed:.2f}s[/dim]")
            
            # ç¡®ä¿responseä¸ä¸ºNone
            if response is None:
                raise RuntimeError("API call succeeded but response is None")
            
            raw_content = response.choices[0].message.content
            
            if not raw_content:
                console.print(f"[bold red][DEBUG] WARNING: LLM returned None or empty content![/bold red]")
                raise ValueError("LLM returned empty response")
            
            # åªåœ¨å‡ºé”™æ—¶æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            # console.print(f"[dim][DEBUG] Raw response: {raw_content[:200]}...[/dim]")
            
            cleaned_content = self._clean_json_response(raw_content)
            
            result = json.loads(cleaned_content)
            # console.print(f"[dim][DEBUG] Successfully parsed JSON with {len(result.get('steps', []))} steps[/dim]")
            
            # éªŒè¯JSONæ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ
            if not isinstance(result, dict):
                console.print(f"[bold red][ERROR] LLM returned invalid format: Expected dict, got {type(result)}[/bold red]")
                console.print(f"[yellow]Raw response:[/yellow]\n{raw_content}")
                raise ValueError(f"LLM returned invalid format: Expected dict, got {type(result)}")
            
            if "steps" not in result:
                console.print(f"[bold red][ERROR] LLM returned JSON without 'steps' field![/bold red]")
                console.print(f"[yellow]Received JSON structure:[/yellow] {list(result.keys())}")
                console.print(f"[yellow]Full response:[/yellow]\n{raw_content}")
                raise ValueError(f"LLM returned JSON without required 'steps' field. Got keys: {list(result.keys())}")
            
            if not isinstance(result.get("steps"), list):
                console.print(f"[bold red][ERROR] 'steps' field is not a list![/bold red]")
                console.print(f"[yellow]Full response:[/yellow]\n{raw_content}")
                raise ValueError(f"'steps' field must be a list, got {type(result.get('steps'))}")
            
            if len(result.get("steps", [])) == 0:
                console.print(f"[bold red][ERROR] LLM returned empty 'steps' list![/bold red]")
                console.print(f"[yellow]Full response:[/yellow]\n{raw_content}")
                raise ValueError("LLM returned empty 'steps' list")
            
            # éªŒè¯æ¯ä¸ªstepçš„æ ¼å¼
            for i, step in enumerate(result["steps"]):
                if not isinstance(step, dict):
                    console.print(f"[bold red][ERROR] Step {i+1} is not a dict![/bold red]")
                    raise ValueError(f"Step {i+1} must be a dict, got {type(step)}")
                if "command" not in step:
                    console.print(f"[bold red][ERROR] Step {i+1} missing 'command' field![/bold red]")
                    console.print(f"[yellow]Step content:[/yellow] {step}")
                    raise ValueError(f"Step {i+1} missing required 'command' field")
            
            return result
            
        except json.JSONDecodeError as e:
            console.print(f"[bold red][DEBUG] JSON Parse Error: {str(e)}[/bold red]")
            console.print(f"[dim][DEBUG] Raw content: {raw_content or 'N/A'}[/dim]")
            raise ValueError(f"LLM returned invalid JSON: {str(e)}")
        except Exception as e:
            elapsed = time.time() - start_time
            console.print(f"[bold red][DEBUG] LLM API Error after {elapsed:.2f}s: {type(e).__name__}: {str(e)}[/bold red]")
            import traceback
            console.print(f"[dim][DEBUG] Traceback:\n{traceback.format_exc()}[/dim]")
            raise RuntimeError(f"LLM API Error: {str(e)}")
    
    def generate_next_steps(
        self,
        user_goal: str,
        context_str: str,
        execution_history: list,
        max_steps: int = 3
    ) -> dict:
        """
        æ ¹æ®å½“å‰çŠ¶æ€ç”Ÿæˆæ¥ä¸‹æ¥çš„æ­¥éª¤ï¼ˆæ¸è¿›å¼æ‰§è¡Œï¼‰
        
        :param user_goal: ç”¨æˆ·çš„æ€»ä½“ç›®æ ‡
        :param context_str: ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
        :param execution_history: å·²æ‰§è¡Œçš„æ­¥éª¤å†å² [{"description": ..., "command": ..., "output": ..., "success": ...}, ...]
        :param max_steps: æœ€å¤šç”Ÿæˆå‡ ä¸ªæ­¥éª¤
        :return: {"thought": ..., "steps": [...], "is_complete": bool}
        """
        
        console.print(f"[dim][DEBUG] Generating next steps (max: {max_steps})...[/dim]")
        start_time = time.time()
        
        # æ„å»ºæ‰§è¡Œå†å²æ‘˜è¦
        history_summary = self._build_history_summary(execution_history)
        
        system_prompt = f"""
You are an expert system engineer with the ability to break down complex tasks into steps and adapt based on execution results.

Current Execution Environment:
{context_str}

âš ï¸ CRITICAL JSON FORMAT REQUIREMENTS âš ï¸

YOU MUST RESPOND WITH **ONLY** A VALID JSON OBJECT IN THIS **EXACT** FORMAT:

{{
   "thought": "Your reasoning about what to do next",
   "steps": [
      {{
         "description": "Step description",
         "command": "shell command"
      }}
   ],
   "is_complete": false
}}

IMPORTANT RULES:
1. Generate 1-{max_steps} steps based on the current situation
2. Consider the execution history and previous outputs
3. Use shell commands for ALL operations (cat, sed, grep, awk, etc.)
4. Set "is_complete": true ONLY when the entire goal is achieved
5. Each step should be atomic and clear
6. Use command substitution and pipes when needed

EXAMPLES OF GOOD COMMANDS:
- Read file: cat ~/test/a.sh
- Check output: if [ "$(cat file.txt)" = "1" ]; then echo "match"; fi
- Edit file: sed -i 's/echo 1/echo 2/g' ~/test/a.sh
- Conditional: [ "$(command)" = "expected" ] && next_command || alternative_command

Remember: Output ONLY the JSON object - absolutely nothing else!
"""

        user_message = f"""User Goal: {user_goal}

{history_summary}

Based on the execution history above, generate the next 1-{max_steps} steps to achieve the goal.

IMPORTANT: You MUST respond with ONLY a JSON object in this exact format:
{{
   "thought": "your reasoning here",
   "steps": [
      {{"description": "step description", "command": "shell command"}}
   ],
   "is_complete": false
}}

Do NOT include any other text, explanations, or markdown. ONLY the JSON object."""

        raw_content = None
        
        try:
            console.print(f"[dim][DEBUG] Calling LLM API for next steps...[/dim]")
            
            api_params = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "temperature": 0.5  # ç¨ä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
            }
            
            # å¤„ç† JSON æ¨¡å¼
            response = None
            
            if self.is_ollama:
                # Ollama: ç›´æ¥è°ƒç”¨
                console.print(f"[dim][DEBUG] Using Ollama for adaptive execution[/dim]")
                response = self.client.chat.completions.create(**api_params)
            else:
                # é Ollama: å°è¯• JSON æ¨¡å¼
                json_mode_failed = False
                
                try:
                    api_params["response_format"] = {"type": "json_object"}
                    response = self.client.chat.completions.create(**api_params)
                except Exception as e:
                    error_msg = str(e)
                    if "response_format" in error_msg or "400" in error_msg:
                        console.print(f"[dim][DEBUG] JSON mode not supported, retrying without it...[/dim]")
                        json_mode_failed = True
                    else:
                        raise
                
                if json_mode_failed:
                    api_params.pop("response_format", None)
                    response = self.client.chat.completions.create(**api_params)
            
            elapsed = time.time() - start_time
            console.print(f"[dim][DEBUG] LLM API responded in {elapsed:.2f}s[/dim]")
            
            if response is None:
                raise RuntimeError("API call succeeded but response is None")
            
            raw_content = response.choices[0].message.content
            
            if not raw_content:
                raise ValueError("LLM returned empty response")
            
            cleaned_content = self._clean_json_response(raw_content)
            result = json.loads(cleaned_content)
            
            # éªŒè¯æ ¼å¼
            if not isinstance(result, dict):
                raise ValueError(f"Expected dict, got {type(result)}")
            
            if "steps" not in result:
                raise ValueError(f"Missing 'steps' field. Got keys: {list(result.keys())}")
            
            if not isinstance(result.get("steps"), list):
                raise ValueError(f"'steps' must be a list, got {type(result.get('steps'))}")
            
            # éªŒè¯æ¯ä¸ªstep
            for i, step in enumerate(result["steps"]):
                if not isinstance(step, dict):
                    raise ValueError(f"Step {i+1} must be a dict, got {type(step)}")
                if "command" not in step:
                    raise ValueError(f"Step {i+1} missing 'command' field")
            
            # ç¡®ä¿ is_complete å­—æ®µå­˜åœ¨
            if "is_complete" not in result:
                result["is_complete"] = False
            
            return result
            
        except json.JSONDecodeError as e:
            console.print(f"[bold red][DEBUG] JSON Parse Error: {str(e)}[/bold red]")
            console.print(f"[dim][DEBUG] Raw content: {raw_content or 'N/A'}[/dim]")
            raise ValueError(f"LLM returned invalid JSON: {str(e)}")
        except Exception as e:
            elapsed = time.time() - start_time
            console.print(f"[bold red][DEBUG] LLM API Error after {elapsed:.2f}s: {type(e).__name__}: {str(e)}[/bold red]")
            import traceback
            console.print(f"[dim][DEBUG] Traceback:\n{traceback.format_exc()}[/dim]")
            raise RuntimeError(f"LLM API Error: {str(e)}")
    
    def _build_history_summary(self, execution_history: list) -> str:
        """æ„å»ºæ‰§è¡Œå†å²æ‘˜è¦"""
        if not execution_history:
            return "Execution History: None (this is the first step)"
        
        summary_parts = ["Execution History:"]
        for i, step in enumerate(execution_history[-10:], 1):  # åªä¿ç•™æœ€è¿‘10æ­¥
            status = "âœ“" if step.get("success") else "âœ—"
            desc = step.get("description", "Unknown")
            cmd = step.get("command", "")
            output = step.get("output", "")
            
            # é™åˆ¶è¾“å‡ºé•¿åº¦
            if output:
                output_preview = output[:200] + "..." if len(output) > 200 else output
                summary_parts.append(f"{i}. {status} {desc}")
                summary_parts.append(f"   Command: {cmd}")
                summary_parts.append(f"   Output: {output_preview}")
            else:
                summary_parts.append(f"{i}. {status} {desc} (Command: {cmd})")
        
        return "\n".join(summary_parts)
